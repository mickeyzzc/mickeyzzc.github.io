<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>指标流聚合系列三：Record Rule维度任务生成器</title><url>/posts/opentelemetry/stream-metrics-three/</url><categories><category>Prometheus</category><category>VictoriaMetrics</category></categories><tags><tag>Prometheus</tag><tag>VictoriaMetrics</tag></tags><content type="html"><![CDATA[  流聚合计算是一种实时处理和分析大量数据流的技术。其主要目的是从高速、连续不断的数据流中提取有价值的信息，例如：计算平均值、求和、最大值、最小值等。流聚合计算通常会在内存中进行，以保持较低的延迟和高吞吐量。
流聚合计算的实现通常包含以下几个关键部分：
数据源和数据接收器：设定和监控数据流，如：日志、网络数据包、传感器数据等，将数据流分为多个小批次。 窗口操作：数据流会被分成一系列固定或滑动窗口。每个窗口内的数据会进行预先定义的聚合操作。 聚合函数：对窗口内的数据进行各种操作,例如：计数、求和、均值、最大/最小值等。 输出：将聚合结果输出或存储，以备进一步处理或实时数据可视化。 在指标流聚合中，该能力是对单一指标做高效的降维计算，但是在现实中要描述复杂场景是需要对多指标来结合函数计算的。
流聚合可以高效解决以下聚合过滤req_path的字段
聚合前： a_http_req_total{zone=&amp;#34;bj&amp;#34;,src_svr=&amp;#34;192.168.1.2&amp;#34;,src_port=&amp;#34;30021&amp;#34;,dis_svr=&amp;#34;192.168.2.3&amp;#34;,dis_port=&amp;#34;8080&amp;#34;,code=&amp;#34;202&amp;#34;,req_path=&amp;#34;/xxxx/yyyy?abc=exg&amp;#34;} a_http_req_total{zone=&amp;#34;bj&amp;#34;,src_svr=&amp;#34;192.168.1.2&amp;#34;,src_port=&amp;#34;30023&amp;#34;,dis_svr=&amp;#34;192.168.2.3&amp;#34;,dis_port=&amp;#34;8080&amp;#34;,code=&amp;#34;202&amp;#34;,req_path=&amp;#34;/xxxx/yyyy?abc=sdf&amp;#34;} a_http_req_total{zone=&amp;#34;bj&amp;#34;,src_svr=&amp;#34;192.168.1.2&amp;#34;,src_port=&amp;#34;10021&amp;#34;,dis_svr=&amp;#34;192.168.2.3&amp;#34;,dis_port=&amp;#34;8080&amp;#34;,code=&amp;#34;202&amp;#34;,req_path=&amp;#34;/xxxx/yyyy?abc=fasdfa&amp;#34;} …  ]]></content></entry><entry><title>指标流聚合系列二：分布式流聚合的网关设计与实现</title><url>/posts/opentelemetry/stream-metrics-two/</url><categories><category>Prometheus</category><category>VictoriaMetrics</category></categories><tags><tag>Prometheus</tag><tag>VictoriaMetrics</tag></tags><content type="html"><![CDATA[  通过《指标流聚合系列一：社区VM对指标流聚合能力分析与问题》分析了目前业界方案结合实际场景遇到的问题分析后，这一节会有针对性的对问题一一解决。 通过源码分析有两个关键部分：
时间窗口的范围： //窗口的范围是间隔设置加上间隔向右位移1位 currentTime := fasttime.UnixTimestamp() deleteDeadline := currentTime + as.intervalSecs + (as.intervalSecs &amp;gt;&amp;gt; 1) 计算窗口的逻辑： // 从缓存读取需聚合的指标 v, ok := as.m.Load(outputKey) if !ok { v = &amp;amp;totalStateValue{ lastValues: make(map[string]*lastValueState), } vNew, loaded := as.m.LoadOrStore(outputKey, v) if loaded { v = vNew } } sv := v.(*totalStateValue) sv.mu.Lock() deleted := sv.deleted if !deleted { // 从聚合后指标的缓存里面查找流入的时间线的记录值 lv, ok := sv.lastValues[inputKey] if !ok { // 如果没有记录值，则取原始值作为记录值 lv = &amp;amp;lastValueState{} sv.lastValues[inputKey] = lv } d := value if ok &amp;amp;&amp;amp; lv.value &amp;lt;= value { // 如果有记录值，则取新值与记录值的差值作为聚合后指标的追加值 d = value - lv.value } if ok || currentTime &amp;gt; as.ignoreInputDeadline { sv.total += d } lv.value = value lv.deleteDeadline = deleteDeadline sv.deleteDeadline = deleteDeadline } 从源码解读，流聚合逻辑只对时间窗口做了简易的周期判断，在流入指标因各种原因导致延迟抵达的问题导致计算值被放大没有太多逻辑处理。  …  ]]></content></entry><entry><title>指标流聚合系列一：社区VM对指标流聚合能力分析与问题</title><url>/posts/opentelemetry/stream-metrics-one/</url><categories><category>Prometheus</category><category>VictoriaMetrics</category></categories><tags><tag>Prometheus</tag><tag>VictoriaMetrics</tag></tags><content type="html"><![CDATA[  一、VictoriaMetrics开源项目的原生能力 VictoriaMetrics项目中的流聚合能力是从1.86版本开始整合到vmagent的，具体可参考： https://github.com/VictoriaMetrics/VictoriaMetrics/issues/3460 从源码分析来看，流集合能力如图：
核心计算的代码在pushSample函数中有描述：
func (as *totalAggrState) pushSample(inputKey, outputKey string, value float64) { currentTime := fasttime.UnixTimestamp() deleteDeadline := currentTime + as.intervalSecs + (as.intervalSecs &gt;&gt; 1) again: v, ok := as.m.Load(outputKey) if !ok { v = &amp;totalStateValue{ lastValues: make(map[string]*lastValueState), } vNew, loaded := as.m.LoadOrStore(outputKey, v) if loaded { v = vNew } } sv := v.(*totalStateValue) sv.mu.Lock() deleted := sv.deleted if !deleted { lv, ok := sv.lastValues[inputKey] if !ok { lv = &amp;lastValueState{} sv.lastValues[inputKey] = lv } d := value if ok &amp;&amp; lv.value &lt;= value { d = value - lv.value } if ok || currentTime &gt; as.ignoreInputDeadline { sv.total += d } lv.value = value lv.deleteDeadline = deleteDeadline sv.deleteDeadline = deleteDeadline } sv.mu.Unlock() if deleted { goto again } } 二、流聚合能力的一般运用分析 首先先看看流聚合后的时序图： 这时候，我们看看时序数据的理论形态 基于理论形态的理想流聚合形态 在实际状态下的流聚合形态 三、原生流聚合能力的日常问题 1、采集断点问题 在现实世界是更残酷的，某个时间断点是无可避免，原因很多，有网络上的、有被采集服务的性能问题等。 高精度的流聚合断点问题在大数计算情况下，影响是很极端的： 2、巨量数据的算力架构 流聚合没有历史数据细节的状态保存，因此性能极优，但是再优性能的服务也存在单点的极限值。在处理单点无法处理的数据量级情况下，分布式算力结构就会存在一系列问题要解决：
vmagent自带采集能力是否可用？ 在实际测试过程中，启动vmagent分片加副本采集能力结合实时流聚合，资源使用大幅增加，在量级极大的情况下频繁出现采集连续断点的问题，恶化了断点导致的计算结果差异问题。 架构图参考：
需计算的Sample数据分别给哪个算力点计算？ 多个算力点计算后的同维度指标集因同维度同时间窗口但不同结果值导致后到值丢弃的问题如何解决？（触发乱序指标处理逻辑） 分布式计算的资源均衡问题？ 插入任务id解决计算点同维度分辨问题后再次引入新维度如何解决？   ]]></content></entry><entry><title>About me</title><url>/about.html</url><categories/><tags/><content type="html"> 在多年工作中，从运维历练到开发，可观测性技术爱好者，对云原生技术也略有经验。 从Shell学起，写了点VBA工具，后来续Python、Lua之后玩起了Golang和Rust。 有过国内外业务平台的运维和架构设计的经历。 用Promtheus+Thanos+VictoriaMetrics+自研架构组件，搭建支持日千亿指标量的跨地域混合云监控平台。
作品 不是很多
Shell+Python 工作需要常写，有写复杂脚本的能力，请点击 MLSBS ,相关的 GIT库 Golang 给Thanos社区和kube-state-metric项目修过Bug，提交了一点点代码
eBPF/Rust 给eBPF项目的代表之一DeepFlow社区的Agent项目补充了MongoDB协议的解码逻辑，用Rust实现。</content></entry><entry><title>ANSIBLE管理线上docker集群</title><url>/posts/old/ansible-docker/</url><categories><category>Ansible</category><category>Docker</category></categories><tags><tag>Ansible</tag><tag>Docker</tag></tags><content type="html"> Docker-Engine的安装: 利用 MickeyZZC / MiAnsibleRules 中的docker-engine ,参考以下playbook:
#!/usr/bin/env ansible-playbook --- - hosts: all gather_facts: yes sudo: yes roles: - role: docker-engine tags: docker Docker-Container: 参考 MickeyZZC / MiAnsibleRules 其他rules. 每个rules都有变量,可以在自己的ansible项目中给予vars值.</content></entry><entry><title>VPS 记录</title><url>/posts/old/vps_oversea/</url><categories><category>Linux</category></categories><tags><tag>Vps</tag><tag>Shadowsocks</tag></tags><content type="html"> 概述： 分类基于虚拟化
KVM 要想启用 BBR 需要切换内核，所以必须要 KVM 或者 XEN 架构的 VPS。 Openvz 开启 BBR （UML或LKL） 虽说 OpenVZ 在正常情况下是无法使用 BBR 的，但是通过其他一些手段还是能够达到目的。比如UML 和 LKL 。 Docker</content></entry><entry><title>Linode的BBR简单测试</title><url>/posts/old/linode-bbr-test/</url><categories><category>Tcp</category></categories><tags><tag>Tcp</tag></tags><content type="html"><![CDATA[  简单阐述: TCP BBR 致力于解决两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟、高带宽的网络链路。 降低网络链路上的 buffer 占用率，从而降低延迟。非常适合慢速接入网络的用户。 测试目的: 这次的测试主要是针对丢包率.更有说服力的测试请参考 Lawrence Brakmo的BBR Report .
BBR的另一面 测试准备： ADDR01：aaa.aaa.aaa.aaa $ uname -r 4.8.6-x86_64-linode78 ADDR02：bbb.bbb.bbb.bbb # uname -r 4.8.6-x86_64-linode78 测试方式： 模拟丢包1%-30%的场景，分别测试不同内核开启BBR先后的情况。 用到的tc指令：
# 清理tc规则： tc qdisc del root dev eth0 # 模拟1%丢包： tc qdisc add dev eth0 root netem loss 1% # 模拟10%丢包： tc qdisc add dev eth0 root netem loss 10% # 模拟30%丢包： tc qdisc add dev eth0 root netem loss 30% 测试从从ADDR02传数据到ADDR01,ADDR01的内核不变,ADDR02在每次测试都会调整内核重启。 测试过程： 步骤略,test.gz约160MB,过程大致如下: 没有启用BBR的情况，从ADDR02传数据到ADDR01：
$ rsync -ave &#34;ssh -l mickey&#34; --progress test.gz mickey@bbb.bbb.bbb.bbb:/home/mickey/test.gz sending incremental file list test.gz 166042909 100% 3.27MB/s 0:00:48 (xfer#1, to-check=0/1) sent 166063274 bytes received 31 bytes 3288382.28 bytes/sec total size is 166042909 speedup is 1.00 测试数据比对: 4.8.6-x86_64-linode78 4.9.15-x86_64-linode78 非linode的官方4.10内核(generic) 没有启动BBR正常情况 3.27MB/s 3.36MB/s 没有测试 启动BBR正常情况 没有测试 3.45MB/s 2.31MB/s 启动BBR丢包1% 3.19MB/s 没有测试 没有测试 启动BBR丢包10% 没有测试 3.21MB/s 2.81MB/s 启动BBR丢包30% 97.30kB/s(在20分钟内没有传输完成中断得到的最后结果) 1.35MB/s 1.15MB/s 测试总结和当时情况(以上述结果来总结): linode自己编译的内核有明显针对性优化,效果比较明显. 启动bbr后在丢包30%的情况下还能完成传输,bbr的效果也比较明显; 4.10内核选择了generic没有选择lowlatency. 本来还打算测50%的丢包.但是50%设置后几乎无法远程操作ADDR01而放弃测试. 附录： centos7官方内核的升级方法： 升级内核: rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install kernel-ml -y 更新启动 egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \&#39; grub2-set-default 0 #default 0表示第一个内核设置为默认运行, 选择最新内核就对了 reboot 开启BBR： modprobe tcp_bbr echo &#34;net.core.default_qdisc=fq&#34; &gt;&gt; /etc/sysctl.conf echo &#34;net.ipv4.tcp_congestion_control=bbr&#34; &gt;&gt; /etc/sysctl.conf sysctl -p   ]]></content></entry><entry><title>树莓派 RaspberryPi docker集群</title><url>/posts/old/rasp_pi_docker/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Arm</tag></tags><content type="html"><![CDATA[  概述： 参考 Hypriot 的博客，我买了1块Rasp2代板和2块Rasp3代板。
其中2代默认安装了 Hypriot 的系统。3代板如果您有兴趣可以自己参考 《Building a 64bit Docker OS for the Raspberry Pi 3》 这篇文章编译一套64bit的系统。也可以直接下载作者的 已编译好镜像地址 中压缩包。
网络的问题： 日常的升级或者包安装之类的情况，都会遇到墙的问题。为了避免经常为墙而烦恼的情况，有必要给控制网络出口的路由做些调整。参考 《Shadowsocks + ChnRoute 实现 OpenWRT / LEDE 路由器自动翻墙》 解决墙的烦恼，因为这个不是这篇文的重点，具体细节略。
给每个arm板子在路由上赋予一个静态IP地址。
系统： OS细节： 个人习惯调整一下环境，如zsh,vim等，可以考虑弄个ansible-playbook或者shell脚本来简化一下。 # 调整默认文本工具为vim $ update-alternatives --config editor $ dpkg-reconfigure locales # 新建自己的账号并加入到docker组 $ usermod a -G docker xxx # 这步对集群很重要，修改设备的名字，否则后期加入集群会报错 $ sed -i &#34;s/black-pearl/node01-pearl/&#34; /boot/device-init.yaml /etc/hostname Kubernetes(本来我想用这个来管理的，但是我的pi2的docker是最新ce版本，kubeadmin提示不支持。) 参考 《Setup Kubernetes on a Raspberry Pi Cluster easily the official way!》 添加源： $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $ echo &#34;deb http://apt.kubernetes.io/ kubernetes-xenial main&#34; &gt; /etc/apt/sources.list.d/kubernetes.list 安装kubeadm $ apt-get update &amp;&amp; apt-get install -y kubeadm 初始化主节点 $ kubeadm init --pod-network-cidr 10.244.0.0/16 略。。。 Swarm(选择了这个) 初始化pi2 $ docker swarm init 把得到的token信息给每个节点接入来 $ docker swarm join \ --token SWMTKN-1-5pm7otmn3vt9bmjhdfdk2hhgxp1zm9wfcyebl7x4dlkbbqujke-4fmuuxxxxxxxxxhiyqem \ 192.168.xx.2:2377 输出一下node信息： $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS mbosd1usr6vfoj2p9zyw6zhau node01-pearl Ready Active sc4vtm5fqbdet8k4vrr38x1fo * master-pearl Ready Active Leader v8uxeq9pc8pzlciing7lol16e node02-pearl Ready Active   ]]></content></entry><entry><title>ARM的点点滴滴记录</title><url>/posts/old/arm-board-note/</url><categories><category>Arm</category></categories><tags><tag>Arm</tag><tag>Linux</tag></tags><content type="html"><![CDATA[  Raspberry Pi 我用树莓派搭建docker集群环境，参考 Hypriot 的博客
CubieBoard 安装docker 选择Hypriot(截止到2017年3月版本是docker 1.11.1) Install dependencies $ apt-get install -y apt-transport-https Add respository key $ wget -q https://packagecloud.io/gpg.key -O - | sudo apt-key add - Add repository $ echo &amp;#39;deb https://packagecloud.io/Hypriot/Schatzkiste/debian/ jessie main&amp;#39; | sudo tee /etc/apt/sources.list.d/hypriot.list $ apt-get update Install Hypriot $ apt-get install -y docker-hypriot $ systemctl enable docker 选择Dockerproject (随官方更新，截止2017年3月，版本17.03-ce) Install dependencies $ sudo apt-get update $ sudo apt-get install apt-transport-https ca-certificates Add repository $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D 用下面的命令将 APT 源添加到 source.list（将其中的 替换为下表“表一：dockerproject 对应表”的值）：
$ echo &amp;#34;&amp;lt;REPO&amp;gt;&amp;#34; | sudo tee /etc/apt/sources.list.d/docker.list Install Docker-Engine $ apt-get update $ apt-get install -y docker-engine …  ]]></content></entry><entry><title>GitHub博客的搭建（docker-hexo）</title><url>/posts/old/blog_mickeyzzc/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Node</tag></tags><content type="html"><![CDATA[  工具选用了 hexo ， 主题选择了 NexT.Mist 。 CDN选择了 CloudFlare 。
过程（详细步骤请参考官方网站，这里只提及过程中的注意点）： 参考 hexo 在本地安装，我用的是debian系统。 在中国境内经常会遇到墙的问题，建议使用 淘宝的cnpm 。
$ npm install -g cnpm --registry=https://registry.npm.taobao.org 然后使用cnpm命令安装hexo-cli
$ cnpm install -g hexo-cli 个性化的自己的配置，并且在github上建立xxx.github.io库。 在 hexo 的本地目录source下初始化git库， 并在自己的git库上管理。
CDN和HTTPS的构建： 在 hexo 的本地目录public下创建CNAME文件，内容为你的域名。 注册 CloudFlare ， 把域名的NS切换到 CloudFlare 管理。 在 CloudFlare 的Crypto页中， SSL设置为Flexible。这将允许CDN到github pages之间的访问为http。 CloudFlare 提供Page Rules功能， 可设置路由规则。通过规则中的Always use https选项，可以将用户强制跳转到https。 http://*.mickeyzzc.tech/* 使用Docker构建 利用DOCKERFILE 构建一个本地使用的hexo镜像. # MAINTAINER MickeyZZC &lt;xxx@xxx.com&gt; # DOCKER-VERSION 1.13.0 # FROM node:6 MAINTAINER MickeyZZC &lt;xxx@xxx.com&gt; RUN cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \ &amp;&amp; npm install -g cnpm --registry=https://registry.npm.taobao.org \ &amp;&amp; cnpm install -g hexo-cli \ &amp;&amp; mkdir -p /home/hexo/public \ &amp;&amp; cd /home/hexo \ &amp;&amp; hexo init \ &amp;&amp; git clone https://github.com/iissnan/hexo-theme-next themes/next \ &amp;&amp; cnpm install hexo-deployer-git --save \ &amp;&amp; git clone https://git.oschina.net/MickeyZZC/MiZDoc.git mickeyblog \ &amp;&amp; cp -f mickeyblog/hexo_config/_config.yml _config.yml \ &amp;&amp; cp -f mickeyblog/hexo_config/themes_next_config.yml themes/next/_config.yml \ &amp;&amp; cp -f mickeyblog/hexo_config/gitconfig.cfg ./.gitconfig \ &amp;&amp; cp -rf mickeyblog/hexo_source/* source/ \ &amp;&amp; rm -rf mickeyblog source/_posts/hello-world.md \ &amp;&amp; chown -R node.node /home/hexo &amp;&amp; chown -R node /usr/local/lib/node_modules/ \ &amp;&amp; echo &#34;blog.mickeyzzc.tech&#34; &gt; /home/hexo/public/CNAME \ &amp;&amp; hexo generate \ &amp;&amp; chown -R node.node /home/hexo ENV HOME /home/hexo WORKDIR /home/hexo EXPOSE 4000 USER node CMD [&#34;hexo&#34;,&#34;server&#34;] 构建后本地运行来调试hexo docker run --rm -p 4000:4000 \ -v $HOME/migit/miBlog:/home/hexo/source\ -it mickeyzzc/node-hexo 最后打包合并到GIT库管理：   ]]></content></entry><entry><title>跨城区局域网的搭建（基于Docker）</title><url>/posts/old/mi-docker-net/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Openvpn</tag></tags><content type="html"><![CDATA[  概述： 管理复杂网络内的系统,有时候需要突破网络限制.有比较多的方案,比如ss5,Shadowsocks,vpn等. 这里提供一种方案是利用 docker-openvpn 实施多重复杂网络内的主机互联,实现利用nginx反向代理各类服务.
概念图： 具体流程： 购买云服务器部署docker 建议购买支持systemctl的Linux系统,比较好管理,并部署docker:
外挂存储格式化为xfs分区; # mkfs.xfs /dev/vdb5 # echo &#34;/dev/vdb5 /mnt/data xfs defaults 1 1&#34; |tee -a /etc/fstab 调整docker的目录,两种方法; 挂载/var/lib/docker目录: # systemctl stop docker # mkdir -p /mnt/data/docker # rsync -aXS /var/lib/docker/. /mnt/data/docker/ # echo &#34;/mnt/data/docker /var/lib/docker none bind 0 0&#34;|tee -a /etc/fstab # mount -a # systemctl start docker 指定具体目录: 在/etc/systemd/system/multi-user.target.wants/docker.service里面修改如下: ExecStart=/usr/bin/dockerd --storage-driver=overlay2 -g /mnt/hhd/docker 安装 docker-openvpn 服务: Pick a name for the $OVPN_DATA data volume container, it will be created automatically. # OVPN_DATA=&#34;ovpn-data&#34; Initialize the $OVPN_DATA container that will hold the configuration files and certificates # docker volume create --name $OVPN_DATA # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_genconfig \ -u udp://VPN.SERVERNAME.COM # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -it kylemanna/openvpn ovpn_initpki 如果使用tcp # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_genconfig \ -u tcp://VPN.SERVERNAME.COM:1443 Start OpenVPN server process # docker run -v $OVPN_DATA:/etc/openvpn -d \ -p 1194:1194/udp \ --cap-add=NET_ADMIN kylemanna/openvpn OR # docker run -v $OVPN_DATA:/etc/openvpn -d \ -p 1443:1194/tcp \ --cap-add=NET_ADMIN kylemanna/openvpn Running a Second Fallback TCP Container # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -p 1443:1194/tcp \ --privileged kylemanna/openvpn ovpn_run \ --proto tcp Generate a client certificate without a passphrase . Retrieve the client configuration with embedded certificates, &ldquo;CLIENTNAME&quot;可自定义;
# docker run -v $OVPN_DATA:/etc/openvpn \ --rm -it kylemanna/openvpn easyrsa build-client-full CLIENTNAME nopass # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_getclient CLIENTNAME &gt; CLIENTNAME.ovpn 增加路由规则 在docker主机上增加一条路由规则,目的是使其他容器可以通过默认的网络来访问到openvpn客户端的节点:
# ip route add 192.168.255.0/24 via $DOCKER_OPENVPN_IP 给客户端配置静态内外IP。 # cat ccd/CLIENTNAME ifconfig-push 192.168.255.10 192.168.255.9 部署前端代理 选择 DOCKER-CADDY 做反向代理: docker run -d \ -v $(pwd)/Caddyfile:/etc/Caddyfile \ -v $HOME/.caddy:/root/.caddy \ -p 80:80 -p 443:443 \ --name caddy \ --link openvpn:openvpn \ abiosoft/caddy CADDY的配置参考： http://git.mickeybee.cn { redir https://git.mickeybee.cn{url} } https://git.mickeybee.cn { gzip proxy / 192.168.xxx.xx:3000 tls xxx@xxx.com { max_certs 10 key_type p256 } } 部署TCP代理 选择 DOCKER-HAPROXY 并部署: docker run -d \ -v $(pwd)/haproxy:/usr/local/etc/haproxy:ro \ -p xxx:xxx -p yyy:yyy \ --name haproxy \ --link openvpn:openvpn \ haproxy 映射后端端口。   ]]></content></entry><entry><title>Tumx + Git + OhMyZsh + VIM</title><url>/posts/old/zsh-tmux-vim-git/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Tumx</tag><tag>Git</tag><tag>Zsh</tag><tag>Vim</tag></tags><content type="html"><![CDATA[  ubuntu下的环境： 要求： tmux &gt;= 2.1 vim &gt;= 7.3 zsh (oh-my-zsh) git 推荐环境： TMUX(使用 gpakosz 的配置)： 部署方式： $ cd ~ $ git clone https://github.com/gpakosz/.tmux.git $ ln -s -f .tmux/.tmux.conf $ cp .tmux/.tmux.conf.local . $ sudo apt-get install xclip ## Ubuntu下安装xclip来支持跨文件复制粘贴 修改“.tmux.conf” 把以下地方修改： bind -t vi-copy y copy-selection 改为 bind -t vi-copy y copy-pipe &ldquo;xclip -sel clip -i&rdquo;
如果tmux &lt;1.8 请修改如下：
# copy &amp; paste between tmux and x clipboard bind C-p run-shell &quot;tmux set-buffer \&quot;$(xclip -o)\&quot;; tmux paste-buffer&quot; bind C-y run-shell &quot;tmux show-buffer | xclip -sel clip -i&quot; 修改“.tmux.conf.local”把以下地方注释去掉： # set -g status-keys vi # set -g mode-keys vi
GIT： 日常都会用到几个git库，有包含同事的和自己的。管理起来都比较麻烦，但是利用到git子模块会比较方便。
$ mkdir xxxx $ git init $ git remote add origin ssh://git@git.xxx.net/mickey/xxxx.git $ git submodule add ssh://git@git.xxx.net/mickey/tttt.git xxxx $ git add -A $ git commit -m &#34;add submodule tttt&#34; $ git push --set-upstream origin master 当需要全部更新的时候：
$ git submodule foreach git pull 初始化:
➜ ~ git clone ssh://git@git.xxx.net/mickey/xxxx.git ➜ ~ cd xxxx ➜ xxxx git:(master) git submodule init ➜ xxxx git:(master) git submodule sync ➜ xxxx git:(master) git submodule update ➜ xxxx git:(master) git submodule foreach git pull origin master   ]]></content></entry></search>