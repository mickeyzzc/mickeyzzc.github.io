<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>eBPF系列之：DeepFlow 扩展协议解析实践（MongoDB协议与Kafka协议）</title><url>/posts/ebpf/deepflow-agent-proto-dev/</url><categories><category>eBPF</category></categories><tags><tag>Tcp</tag><tag>DeepFlow</tag><tag>eBPF</tag></tags><content type="html"> 概述： 如何分析一个协议(MongoDB) 协议文档的分析思路 MongoDB协议操作码说明表 对最常见的操作码OP_MSG分析 在DeepFlow Agent扩展一个协议解析采集 DeepFlow Agent的开发文档概要 代码指引 定义一个协议，并用一个常量标识。 为新协议准备解析逻辑 定义结构体 实现 L7ProtocolParserInterface 利用Wasm插件扩展DeepFlow的协议采集 Kafka协议分析 Kafka的Header和Data概览 Kafka的Fetch API Kafka的Produce API Kafka协议DeepFlow Agent原生解码 DeepFlow Agent的 Wasm 插件 Wasm Go SDK 的框架 插件代码指引 结语 原生Rust扩展 Wasm插件扩展 附录 概述： MongoDB 目前使用广泛，但是缺乏有效的可观测能力。 DeepFlow 在可观测能力上是很优秀的解决方案，但是却缺少了对 MongoDB 协议的支持。 该文是为 DeepFlow 扩展了 MongoDB 协议解析，增强 MongoDB 生态的可观测能力，简要描述了从协议文档分析到在 DeepFlow 内实现代码解析的过程拆解。
如何分析一个协议(MongoDB) 协议文档的分析思路 首先要从官方网站找到协议解析的文档，在协议文档 《mongodb-wire-protocol#standard-message-header》 中，可以看到MongoDB的协议头结构体描述如下：
1 2 3 4 5 6 7 struct MsgHeader { int32 messageLength; // total message size, including this int32 requestID; // identifier for this message int32 responseTo; // requestID from the original request // (used in responses from the database) int32 opCode; // message type } 上述结构代码理解为下图所示：
注意，在协议文档 《mongodb-wire-protocol》 有一段说明，MongoDB协议是用了 …</content></entry><entry><title>VictoriaMetrics的指标流聚合能力应用</title><url>/posts/opentelemetry/stream-metrics-one/</url><categories><category>Prometheus</category><category>VictoriaMetrics</category></categories><tags><tag>Prometheus</tag><tag>VictoriaMetrics</tag></tags><content type="html"><![CDATA[  社区VM对指标流聚合能力分析与问题 VictoriaMetrics开源项目的原生能力 VictoriaMetrics项目中的流聚合能力是从1.86版本开始整合到vmagent的，具体可参考： https://github.com/VictoriaMetrics/VictoriaMetrics/issues/3460 从源码分析来看，流集合能力如图：
核心计算的代码在pushSample函数中有描述：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 func (as *totalAggrState) pushSample(inputKey, outputKey string, value float64) { currentTime := fasttime.UnixTimestamp() deleteDeadline := currentTime + as.intervalSecs + (as.intervalSecs &amp;gt;&amp;gt; 1) again: v, ok := as.m.Load(outputKey) if !ok { v = &amp;amp;totalStateValue{ lastValues: make(map[string]*lastValueState), } vNew, loaded := as.m.LoadOrStore(outputKey, v) if loaded { v = vNew } } sv := v.(*totalStateValue) sv.mu.Lock() deleted := sv.deleted if !deleted { lv, ok := sv.lastValues[inputKey] if !ok { lv = &amp;amp;lastValueState{} sv.lastValues[inputKey] = lv } d := value if ok &amp;amp;&amp;amp; lv.value &amp;lt;= value { d = value - lv.value } if ok || currentTime &amp;gt; …  ]]></content></entry><entry><title>eBPF系列之：Pixie浅剖析</title><url>/posts/ebpf/pixie-try/</url><categories><category>eBPF</category></categories><tags><tag>eBPF</tag></tags><content type="html"> 部署过程和指令参考： pixie install Pixie平台主要由以下组件组成： Pixie Edge Module 边缘模块(PEM): Pixie&amp;rsquo;s agent, installed per node. PEMs use eBPF to collect data, which is stored locally on the node. Pixie的代理，安装在每个节点上。PEM使用eBPF收集数据，这些数据存储在节点本地。
Vizier: Pixie’s collector, installed per cluster. Responsible for query execution and managing PEMs. Pixie的收集器，安装在每个集群上。负责查询执行和管理PEM。
Pixie Cloud: Used for user management, authentication, and data proxying. Can be hosted or self-hosted. 用于用户管理、身份验证和数据代理。可以托管或自托管。
Pixie CLI: Used to deploy Pixie. Can also be used to run queries and manage resources like API keys. 用于部署Pixie。还可用于运行查询和管理API密钥等资源。
Pixie Client API: Used for programmatic access to Pixie (e.g. integrations, Slackbots, and custom user logic requiring Pixie data as an input) 用于对Pixie进行编程访问（例如，集成、Slackbots和需要Pixie数据作为输入的自定义用户逻辑）
本地部署涉及的组件和步骤： 初始化 日常运行时 Pixie自动收集以下数据：
Protocol traces: Full-body messages between the pods of your applications. Tracing currently supports the following list of protocols . For more information, see the Request Tracing , Service Performance , and Database Query Profiling tutorials.
Resource metrics 资源指标: CPU, memory and I/O metrics for your pods. For more information, see the Infra Health tutorial. 单元的CPU、内存和I/O指标。有关详细信息，请参见 基础架构运行状况 教程.
Network metrics 网络度量: Network-layer and connection-level RX/TX statistics. For more information, see the Network Monitoring tutorial. 网络层和连接级RX/TX统计信息。有关详细信息，请参阅 网络监视 教程。
JVM metrics: JVM memory management metrics for Java applications. Java应用程序的JVM内存管理度量。
Application CPU profiles: Sampled stack traces from your application. Pixie的连续性能分析器始终处于运行状态，以便在您需要时帮助识别应用程序的性能瓶颈。目前支持编译语言（Go、Rust、C/C ++）。有关详细信息，请参阅 Continuous Application Profiling 教程.
私有云端部署组件及架构： 涉及到的镜像： olm组件管理pixie 该组件部署要重制索引镜像指向国内仓库，并在国内仓库一一完成镜像指向配置。 elastic-operator 部署 elastic集群 nats集群组件 postgres gcr.io/pixie-oss/pixie-dev/cloud/api_server_image gcr.io/pixie-oss/pixie-dev/cloud/artifact_tracker_server_image gcr.io/pixie-oss/pixie-dev/cloud/auth_server_image gcr.io/pixie-oss/pixie-dev/cloud/config_manager_server_image gcr.io/pixie-oss/pixie-dev/cloud/proxy_server_image gcr.io/pixie-oss/pixie-dev/cloud/indexer_server_image gcr.io/pixie-oss/pixie-dev/cloud/metrics_server_image gcr.io/pixie-oss/pixie-dev/cloud/plugin_server_image gcr.io/pixie-oss/pixie-dev/cloud/profile_server_image gcr.io/pixie-oss/pixie-dev/cloud/project_manager_server_image gcr.io/pixie-oss/pixie-dev/cloud/scriptmgr_server_image gcr.io/pixie-oss/pixie-dev/cloud/cron_script_server_image gcr.io/pixie-oss/pixie-dev/cloud/vzconn_server_image gcr.io/pixie-oss/pixie-dev/cloud/vzmgr_server_image gcr.io/pixie-oss/pixie-dev/cloud/plugin/load_db 用户管理，脚本管理，任务执行管理记录在Postgres，Es用于缓存计算。</content></entry><entry><title>闲聊一下CPU时序和现代操作系统二三事</title><url>/posts/opentelemetry/talk-about-cpu-timer/</url><categories><category>Linux</category><category>CPU</category></categories><tags><tag>Linux</tag></tags><content type="html"> 时分系统和Linux 首先我们补习一下时分系统，时分系统是一个非常重要的操作系统概念,它最大限度地提高了运算机的利用率,是实现多道程序并发执行的重要手段。 我们日常工作用到的Linux系统 内核也采用了时分系统的思想,主要体现在以下几个方面:
时间片: Linux 使用时间片机制对 CPU 进行时间分割,每个进程只能执行一个时间片的时间,然后交出 CPU 给其他进程运行。这实现了 CPU 时间的共享与公平分配。
上下文切换: 当时间片用完或进程主动放弃 CPU 时,会进行上下文切换,保存当前进程上下文并恢复下一个进程上下文。这使得 CPU 可以高效地在不同进程间切换。
进程调度: Linux 使用 CFS 调度器根据每个进程的时间片选择最适合运行的进程,这是时分系统思想的体现。不同的调度策略可以实现不同的时分效果。
中断机制: Linux 使用中断机制实现对时间的管理与调度。时钟中断可以在时间片用完时通知内核进行上下文切换与调度。这为时分系统提供了动力基础。
时钟事件: Linux 基于时分系统管理各种时间事件,如定时器、睡眠唤醒等。这需要内核根据时钟中断来进行管理与调度。
除此之外,时分系统的思想在 Linux 中还体现在:
多道程序设计 Linux 支持多道程序并发运行,这也依赖于时分系统实现的 CPU 时间共享机制。 实时性 通过设置实时调度策略和对中断处理的优化,Linux 可以提供较好的实时响应性能。这也需要时分系统的支持。 睡眠唤醒 进程可以主动睡眠释放 CPU,这需要时分系统在其唤醒后重新调度其 CPU 时间。 同步机制 Linux 提供多种同步机制,这都需要时分系统来实现进程之间的协调与调度。 时分系统是 Linux 实现多道程序、并发执行、实时响应、时间管理等功能的基础。它使 Linux 能够充分利用 CPU 资源,实现高效率与公平的调度。时分系统的思想贯穿 Linux 内核的方方面面,是理解 Linux 调度与实现并发执行的重要概念。
聊一下Linux的时序 接下来我们需要提前了解几个概念：
Some Words Jiffies 有更多兴趣的可以看看《 内核时钟问题 》
jiffies是一个非常重要的Linux内核变量,它代表了自系统启动以来的时间戳,以时钟中断的个数来表示。它有以下特点:
jiffies以时钟中断的个数来衡量时间,所以它的精度由 …</content></entry><entry><title>监控系统企业架构演进史-拨测监控</title><url>/posts/opentelemetry/prometheus-evolution-history-three/</url><categories/><tags/><content type="html"> 前情概述： 在《监控系统企业架构演进史-跨地域混合云》中，监控系统已经逐步成熟且企业化发展。 这一章节简单讲述一下期间的拨测能力搭建，以下是这套系统的发展史，在监控平台搭建的过程中，内部监控采集还不足以满足企业业务需求，在计划发展apm之前，异地拨测的黑匣子监控也纳入了该系统的一个子功能。
拨测监控架构的实现 系统搭建⾯临需解决的问题：
⻓期以来企业在公⽹监控，乃⾄⽤⼾侧最后⼀公⾥的监控都存在空洞，导致⽤⼾侧的业务故障问题 企业都没有及时发现，需要⽤⼾报障我们才后知后觉的排查问题。⿊匣⼦拨测监控系统项⽬的上线 就是解决了这⻓期以来的监控痛点。 ⿊盒监控 即以⽤⼾的⾝份测试服务的外部可⻅性，常⻅的 ⿊盒监控 包括 HTTP探针 、 TCP探针 等⽤于检测 站点或者服务的可访问性 ，以及 访问效率 等。⽽探针的设计需要⽀持对业务的交互 才能更有效的发现问题。所以在探针⼯具选型中选择了 Prometheus + blackbox_exporter 来实现需求。 拨测点需要在全国各地布点，在管理上难度较⼤，特别在兼顾拨测任务分发、拨测监控数据回收统 ⼀展⽰、告警聚合收敛的同时，还要考虑安全和应对审计等问题。架构的设计上需要严格控制 PULL和PUSH的数据流，还要和现有的采集监控系统独⽴出来。所以引⼊了Mosn做⽹格管理来降 低管理成本。 该系统的数据展⽰上默认只能⽤时序图和表格来展⽰现状，⼀个很直⽩的地域图更能说明问题。为 了做地域图的展⽰，还引进了 geohash + OpenStreetMap 来解决。 需求与功能 第一期的建设只是基本要求，但是需满足以下条件以达成业务基本需求：
⽀持对公司前端服务的证书链，DNS耗时，TLS耗时，⾸次建⽴建⽴耗时，加载完成耗时等监控 ⽀持ICMP拨测，可对⽣产业务系统的跨域内外⽹的⽹络质量监控，特别是跨域专线质量的监 控。 ⽀持DNS，TLS tcp，SMTP协议等交互监控。均⽀持在CDN服务场景，Proxy服务基础场景，邮 箱系统的使⽤场景。 同时，因为项目的0-1阶段基本都很难得到企业的深度投入，在第一期的建设也只能依赖开源项目搭建，后续逐步投入组件二开的研发资源以实现能力扩展。
架构简要 首先，定义每个拨测点为边缘孤岛，这里的边缘孤岛是因为它部署的地理位置远离企业系统的机房，分别在世界各地购买一些最便宜的虚拟机资源来部署服务， …</content></entry><entry><title>监控系统企业架构演进史-跨地域混合云</title><url>/posts/opentelemetry/prometheus-evolution-history-two/</url><categories/><tags/><content type="html"> 前情概述： 在《监控系统企业架构演进史-初入Prometheus》中，监控系统已经从单体架构升级到单IDC分布式架构了。 前一篇文章的内容是适用于虚拟机部署和容器部署的。Prometheus是云原生时代的产物，一般和Kubernetes配套使用，但是Prometheus本身也能在非Kubernetes取替传统监控如Zabbix使用的。 在该篇文章中，开始以Kubernetes的部署来升级整个监控系统架构，使之在跨地域混合云的业务场景中更具灵活性。
架构设计 跨地域的三层结构设计 设计三层区域结构，同时规范区域命名标签化来实现快速辨识服务的地域详细信息。 在第三层中的Cluster和VPC是同级，分别代表集群内或者某网段的隔离服务。
前端查询入口逻辑架构 用Thanos Query实现前期的层级架构 利用Thanos内的GRPC通讯协议和聚合查询能力来实现递级数据汇聚到最上层Thanos Query组件，再聚合计算时间线结果集前端展示。
引入Thanos Query Frontend完成统一前端查询入口 Thanos Query Frontend组件有以下配置能力优化查询，需根据实际情况调整：
时间线的纵向切割查询 比如查15天的数据，由于样本量的数据庞大，会在原始数据读取到内存时导致OOM问题。通过纵向切割比如把15天的聚合查询逻辑拆分成每6小时的聚合查询。 Thanos Query组件就会得到4 * 15个并发查询去完成样本查询并聚合成不同时间段的结果集再拼接展示，且每完成一个子查询聚合均及时释放内存高效优化了资源利用率。
查询结果集缓存 通过对查询语句和时间周期的HASH KEY缓存结果集到内存或者Redis以重复利用，减轻上游压力。
利用Kubernetes赋予更具弹性的冗余能力 自研架构组件 在基于原生开源项目的基础架构下，已基本实现对跨地域混合云的能力。但是要做到企业日常管理还远远不够，需要完善管理架构和前台能力才称得上企业服务。
基础设计逻辑 为了让整个架构具备灵活性和通用性，分别设计了几个组件：
Self-research service discovery 用于对接第三方系统，比如CMDB，CICD等收集业务系统和资产信息，并计算各个业务系统和基建关联关系，通过地域信息来调度资源信息同步给P-sidcar组件。 P-sidcar用于在边缘管 …</content></entry><entry><title>监控系统企业架构演进史-初入Prometheus</title><url>/posts/opentelemetry/prometheus-evolution-history-one/</url><categories/><tags/><content type="html"> Prometheus是一个开源的监控与时间序列数据库系统,在近年来得到了越来越广泛的应用。 官方的架构图如图所示： 本系列文章会以Prometheus的在一个企业里的部署架构演进过程中逐步理解和深入各种组件和概念。 先通过下图简单了解这个演进发展史 单节点架构 刚开始接触Prometheus监控体系，只需要在服务端部署Prometheus的二进制文件，用最基础的文件服务发现配置file_sd_config来实现对主机基础监控node_exporter进行拉取指标采集即可。 再通过Grafana的datasource配置Prometheus的url地址即可开始配置查看监控数据。
指标数据采集 Prometheus的数据模型主要由Metric、Label和 Sample组成。
Metric: 表示一个时序指标,对应于一个监控指标名称。如cpu_usage、free_memory等。 Metric仅包含时序数据名称,没有预定义的结构或类型。这使Prometheus具有很高的灵活性。 一个Prometheus实例可以包含任意数量的Metric。 Label: 用来描述和区分相同Metric的数据。类似于其他时序数据库的Tag。 Label通常表示数据的维度或属性,如instance、job、region等。 每个样本数据都必须包含相同的Label集。Label用于快速查询和聚合特定维度的数据。 Label的值可以是字符串、布尔值或整数。支持在Label上过滤和分组数据。 Sample: 表示一条时序数据,包含Timestamp、Value和Label集。 Timestamp表示时序数据的时间戳,精度为毫秒。它用于排序和查询给定时间范围的数据。 Value表示时序指标的值,可以是浮点数、整数或字符串。 Label集用于标识该Sample数据的属性与维度。相同Label的Sample表示同一指标的不同记录。 一个Prometheus Sample包含:
1 2 3 4 Metric - 时序指标名称 Timestamp - 时间戳,毫秒精度 Value - 指标数值 Label - 数据属性集 例如:
1 2 3 4 5 6 7 8 9 10 11 ################################################### Metric: …</content></entry><entry><title>About me</title><url>/about.html</url><categories/><tags/><content type="html"> 在多年工作中，从运维历练到开发，可观测性技术爱好者，对云原生技术也略有经验。 从Shell学起，写了点VBA工具，后来续Python、Lua之后玩起了Golang和Rust。 有过国内外业务平台的运维和架构设计的经历。 用Promtheus+Thanos+VictoriaMetrics+自研架构组件，搭建支持日千亿指标量的跨地域混合云监控平台。
作品 不是很多
Shell+Python 工作需要常写，有写复杂脚本的能力，请点击 MLSBS ,相关的 GIT库 Golang 给Thanos社区和kube-state-metric项目修过Bug，提交了一点点代码
eBPF/Rust 给eBPF项目的代表之一DeepFlow社区的Agent项目补充了MongoDB协议的解码逻辑，用Rust实现。</content></entry><entry><title>Linode的BBR简单测试</title><url>/posts/old/linode-bbr-test/</url><categories><category>Tcp</category></categories><tags><tag>Tcp</tag></tags><content type="html"><![CDATA[  概述: TCP BBR 致力于解决两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟、高带宽的网络链路。 降低网络链路上的 buffer 占用率，从而降低延迟。非常适合慢速接入网络的用户。 测试目的: 这次的测试主要是针对丢包率.更有说服力的测试请参考 Lawrence Brakmo的BBR Report . BBR的另一面 测试准备： ADDR01：aaa.aaa.aaa.aaa 1 2 $ uname -r 4.8.6-x86_64-linode78 ADDR02：bbb.bbb.bbb.bbb 1 2 # uname -r 4.8.6-x86_64-linode78 测试方式： 模拟丢包1%-30%的场景，分别测试不同内核开启BBR先后的情况。 用到的tc指令： 1 2 3 4 5 6 7 8 # 清理tc规则： tc qdisc del root dev eth0 # 模拟1%丢包： tc qdisc add dev eth0 root netem loss 1% # 模拟10%丢包： tc qdisc add dev eth0 root netem loss 10% # 模拟30%丢包： tc qdisc add dev eth0 root netem loss 30% 测试从从ADDR02传数据到ADDR01,ADDR01的内核不变,ADDR02在每次测试都会调整内核重启。 测试过程： 步骤略,test.gz约160MB,过程大致如下: 没有启用BBR的情况，从ADDR02传数据到ADDR01： 1 2 3 4 5 6 $ rsync -ave &#34;ssh -l mickey&#34; --progress test.gz mickey@bbb.bbb.bbb.bbb:/home/mickey/test.gz sending incremental file list test.gz 166042909 100% 3.27MB/s 0:00:48 (xfer#1, to-check=0/1) sent 166063274 bytes received 31 bytes 3288382.28 bytes/sec total size is 166042909 speedup is 1.00 测试数据比对: 4.8.6-x86_64-linode78 4.9.15-x86_64-linode78 非linode的官方4.10内核(generic) 没有启动BBR正常情况 3.27MB/s 3.36MB/s 没有测试 启动BBR正常情况 没有测试 3.45MB/s 2.31MB/s 启动BBR丢包1% 3.19MB/s 没有测试 没有测试 启动BBR丢包10% 没有测试 3.21MB/s 2.81MB/s 启动BBR丢包30% 97.30kB/s(在20分钟内没有传输完成中断得到的最后结果) 1.35MB/s 1.15MB/s 测试总结和当时情况(以上述结果来总结): linode自己编译的内核有明显针对性优化,效果比较明显. 启动bbr后在丢包30%的情况下还能完成传输,bbr的效果也比较明显; 4.10内核选择了generic没有选择lowlatency. 本来还打算测50%的丢包.但是50%设置后几乎无法远程操作ADDR01而放弃测试. 附录： centos7官方内核的升级方法： 升级内核: 1 2 3 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install kernel-ml -y 更新启动 1 2 3 egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \&#39; grub2-set-default 0 #default 0表示第一个内核设置为默认运行, 选择最新内核就对了 reboot 开启BBR： 1 2 3 4 modprobe tcp_bbr echo &#34;net.core.default_qdisc=fq&#34; &gt;&gt; /etc/sysctl.conf echo &#34;net.ipv4.tcp_congestion_control=bbr&#34; &gt;&gt; /etc/sysctl.conf sysctl -p   ]]></content></entry><entry><title>树莓派 RaspberryPi docker集群</title><url>/posts/old/rasp_pi_docker/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Arm</tag></tags><content type="html"><![CDATA[  概述： 参考 Hypriot 的博客，我买了1块Rasp2代板和2块Rasp3代板。
其中2代默认安装了 Hypriot 的系统。3代板如果您有兴趣可以自己参考 《Building a 64bit Docker OS for the Raspberry Pi 3》 这篇文章编译一套64bit的系统。也可以直接下载作者的 已编译好镜像地址 中压缩包。
网络的问题： 日常的升级或者包安装之类的情况，都会遇到墙的问题。为了避免经常为墙而烦恼的情况，有必要给控制网络出口的路由做些调整。参考 《Shadowsocks + ChnRoute 实现 OpenWRT / LEDE 路由器自动翻墙》 解决墙的烦恼，因为这个不是这篇文的重点，具体细节略。
给每个arm板子在路由上赋予一个静态IP地址。
系统： OS细节： 个人习惯调整一下环境，如zsh,vim等，可以考虑弄个ansible-playbook或者shell脚本来简化一下。 1 2 3 4 5 6 7 # 调整默认文本工具为vim $ update-alternatives --config editor $ dpkg-reconfigure locales # 新建自己的账号并加入到docker组 $ usermod a -G docker xxx # 这步对集群很重要，修改设备的名字，否则后期加入集群会报错 $ sed -i &#34;s/black-pearl/node01-pearl/&#34; /boot/device-init.yaml /etc/hostname Kubernetes(本来我想用这个来管理的，但是我的pi2的docker是最新ce版本，kubeadmin提示不支持。) 参考 《Setup Kubernetes on a Raspberry Pi Cluster easily the official way!》 添加源：
1 2 $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $ echo &#34;deb http://apt.kubernetes.io/ kubernetes-xenial main&#34; &gt; /etc/apt/sources.list.d/kubernetes.list 安装kubeadm
1 $ apt-get update &amp;&amp; apt-get install -y kubeadm 初始化主节点
1 $ kubeadm init --pod-network-cidr 10.244.0.0/16 略。。。
Swarm(选择了这个)
初始化pi2
1 $ docker swarm init 把得到的token信息给每个节点接入来
1 2 3 $ docker swarm join \ --token SWMTKN-1-5pm7otmn3vt9bmjhdfdk2hhgxp1zm9wfcyebl7x4dlkbbqujke-4fmuuxxxxxxxxxhiyqem \ 192.168.xx.2:2377 输出一下node信息：
1 2 3 4 5 $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS mbosd1usr6vfoj2p9zyw6zhau node01-pearl Ready Active sc4vtm5fqbdet8k4vrr38x1fo * master-pearl Ready Active Leader v8uxeq9pc8pzlciing7lol16e node02-pearl Ready Active Docker-Engine的安装: 利用 MickeyZZC / MiAnsibleRules 中的docker-engine ,参考以下playbook:
1 2 3 4 5 6 7 8 #!/usr/bin/env ansible-playbook --- - hosts: all gather_facts: yes sudo: yes roles: - role: docker-engine tags: docker Docker-Container: 参考 MickeyZZC / MiAnsibleRules 其他rules. 每个rules都有变量,可以在自己的ansible项目中给予vars值.   ]]></content></entry><entry><title>ARM的点点滴滴记录</title><url>/posts/old/arm-board-note/</url><categories><category>Arm</category></categories><tags><tag>Arm</tag><tag>Linux</tag></tags><content type="html"><![CDATA[  Raspberry Pi 我用树莓派搭建docker集群环境，参考 Hypriot 的博客
CubieBoard 安装docker
选择Hypriot(截止到2017年3月版本是docker 1.11.1)
Install dependencies 1 $ apt-get install -y apt-transport-https Add respository key 1 $ wget -q https://packagecloud.io/gpg.key -O - | sudo apt-key add - Add repository 1 2 $ echo &amp;#39;deb https://packagecloud.io/Hypriot/Schatzkiste/debian/ jessie main&amp;#39; | sudo tee /etc/apt/sources.list.d/hypriot.list $ apt-get update Install Hypriot 1 2 $ apt-get install -y docker-hypriot $ systemctl enable docker 选择Dockerproject (随官方更新，截止2017年3月，版本17.03-ce)
Install dependencies 1 2 $ sudo apt-get update $ sudo apt-get install apt-transport-https ca-certificates Add repository 1 $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D 用下面的命令将 APT 源添加到 source.list（将其中的 替换为下表“表一：dockerproject 对应表”的值）：
1 $ echo &amp;#34;&amp;lt;REPO&amp;gt;&amp;#34; | sudo tee /etc/apt/sources.list.d/docker.list Install Docker-Engine 1 2 $ apt-get update $ apt-get …  ]]></content></entry><entry><title>GitHub博客的搭建</title><url>/posts/old/blog-mickeyzzc-github/</url><categories><category>GitHub</category></categories><tags><tag>GitHub</tag></tags><content type="html"><![CDATA[  2023年的博客从hexo切换到hugo 工具选用了 hugo 主题选择了 Hugo.NexT CDN选择了 CloudFlare 迁移过程 搭建hugo新环境 1 2 3 4 5 6 7 hugo new site mickeyzzcblog cd mickeyzzcblog git init git submodule add https://github.com/hugo-next/hugo-theme-next.git themes/hugo-theme-next cp themes/hugo-theme-next/exampleSite/config.yaml . vim config.yaml hugo server 构建github ci的workflows 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 name: deploy on: push: workflow_dispatch: schedule: # Runs everyday at 8:00 AM - cron: &#34;0 0 * * *&#34; jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: &#34;latest&#34; extended: true - name: Build Web run: hugo - name: Deploy Web uses: peaceiris/actions-gh-pages@v3 with: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} EXTERNAL_REPOSITORY: mickeyzzc/mickeyzzc.github.io PUBLISH_BRANCH: main PUBLISH_DIR: ./public commit_message: ${{ github.event.head_commit.message }} 2017年的博客建设 工具选用了 hexo 主题选择了 NexT.Mist CDN选择了 CloudFlare 本地搭建hexo环境 详细步骤请参考官方网站，这里只提及过程中的注意点
参考 hexo 在本地安装，我用的是debian系统。 在中国境内经常会遇到墙的问题，建议使用 淘宝的cnpm 。 1 $ npm install -g cnpm --registry=https://registry.npm.taobao.org 然后使用cnpm命令安装hexo-cli
1 $ cnpm install -g hexo-cli 个性化的自己的配置，并且在GitHub上建立xxx.github.io库。 在 hexo 的本地目录source下初始化git库，并在自己的git库上管理。 配置CDN和HTTPS 在 hexo 的本地目录public下创建CNAME文件，内容为你的域名。 注册 CloudFlare ， 把域名的NS切换到 CloudFlare 管理。 在 CloudFlare 的Crypto页中， SSL设置为Flexible。这将允许CDN到github pages之间的访问为http。 CloudFlare 提供Page Rules功能， 可设置路由规则。通过规则中的Always use https选项，可以将用户强制跳转到https。 http://*.mickeyzzc.tech/* 使用Docker构建Blog静态文件 利用DOCKERFILE构建一个本地使用的hexo镜像. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # MAINTAINER MickeyZZC &lt;xxx@xxx.com&gt; # DOCKER-VERSION 1.13.0 # FROM node:6 MAINTAINER MickeyZZC &lt;xxx@xxx.com&gt; RUN cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \ &amp;&amp; npm install -g cnpm --registry=https://registry.npm.taobao.org \ &amp;&amp; cnpm install -g hexo-cli \ &amp;&amp; mkdir -p /home/hexo/public \ &amp;&amp; cd /home/hexo \ &amp;&amp; hexo init \ &amp;&amp; git clone https://github.com/iissnan/hexo-theme-next themes/next \ &amp;&amp; cnpm install hexo-deployer-git --save \ &amp;&amp; git clone https://git.oschina.net/MickeyZZC/MiZDoc.git mickeyblog \ &amp;&amp; cp -f mickeyblog/hexo_config/_config.yml _config.yml \ &amp;&amp; cp -f mickeyblog/hexo_config/themes_next_config.yml themes/next/_config.yml \ &amp;&amp; cp -f mickeyblog/hexo_config/gitconfig.cfg ./.gitconfig \ &amp;&amp; cp -rf mickeyblog/hexo_source/* source/ \ &amp;&amp; rm -rf mickeyblog source/_posts/hello-world.md \ &amp;&amp; chown -R node.node /home/hexo &amp;&amp; chown -R node /usr/local/lib/node_modules/ \ &amp;&amp; echo &#34;blog.mickeyzzc.tech&#34; &gt; /home/hexo/public/CNAME \ &amp;&amp; hexo generate \ &amp;&amp; chown -R node.node /home/hexo ENV HOME /home/hexo WORKDIR /home/hexo EXPOSE 4000 USER node CMD [&#34;hexo&#34;,&#34;server&#34;] 构建后本地运行来调试hexo 1 2 3 docker run --rm -p 4000:4000 \ -v $HOME/migit/miBlog:/home/hexo/source\ -it mickeyzzc/node-hexo 最后打包合并到GIT库管理 1 2 3 # git add -A # git commit -a -s -m &#34;xxxx&#34; # git push   ]]></content></entry><entry><title>监控采集点点记录</title><url>/posts/opentelemetry/monitor-experience/</url><categories><category>Monitor</category></categories><tags><tag>Mysql</tag><tag>Tcp</tag><tag>Linux</tag></tags><content type="html"><![CDATA[  MYSQL的监控 MySQL权限经验原则 权限控制主要是出于安全因素，因此需要遵循一下几个经验原则：
只授予能满足需要的最小权限，防止用户干坏事。比如用户只是需要查询，那就只给select权限就可以了，不要给用户赋予update、insert或者delete权限。 创建用户的时候限制用户的登录主机，一般是限制成指定IP或者内网IP段。 初始化数据库的时候删除没有密码的用户。安装完数据库的时候会自动创建一些用户，这些用户默认没有密码。 为每个用户设置满足密码复杂度的密码。 定期清理不需要的用户。回收权限或者删除用户。 eg:
针对MYSQL的监控需要开监控账号，针对本地监控和远程监控的分别授权；
1 2 GRANT USAGE,PROCESS,REPLICATION CLIENT,REPLICATION SLAVE ON *.* TO &#39;monitor&#39;@&#39;10.12.%&#39; IDENTIFIED BY &#39;xxx&#39;; GRANT USAGE,SUPER,PROCESS,REPLICATION CLIENT,REPLICATION SLAVE ON *.* TO &#39;monitor&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;xxx&#39;; 监控手段 show global status; 查看全局状态
show global variables; 查看全局变量设置
mysqladmin MySQL管理工具
show master status; 查看Master状态
show slave status; 查看Slave状态
show binary logs; 查看二进制日志文件
show engine innodb status\G 查看InnoDB存储引擎状态
show engine myisam status\G 查看MyISAM存储引擎状态
还有通过查看information_schema 这个数据库获取InnoDB存储引擎相关信息
权限表 权限 权限级别 权限说明 CREATE 数据库、表或索引 创建数据库、表或索引权限 DROP 数据库或表 删除数据库或表权限 GRANT OPTION 数据库、表或保存的程序 赋予权限选项 REFERENCES 数据库或表 ALTER 表 更改表，比如添加字段、索引等 DELETE 表 删除数据权限 INDEX 表 索引权限 INSERT 表 插入权限 SELECT 表 查询权限 UPDATE 表 更新权限 CREATE VIEW 视图 创建视图权限 SHOW VIEW 视图 查看视图权限 ALTER ROUTINE 存储过程 更改存储过程权限 CREATE ROUTINE 存储过程 创建存储过程权限 EXECUTE 存储过程 执行存储过程权限 FILE 服务器主机上的文件访问 文件访问权限 CREATE TEMPORARY TABLES 服务器管理 创建临时表权限 LOCK TABLES 服务器管理 锁表权限 CREATE USER 服务器管理 创建用户权限 PROCESS 服务器管理 查看进程权限 RELOAD 服务器管理 执行flush-hosts, flush-logs, flush-privileges, flush-status, flush-tables, flush-threads, refresh, reload等命令的权限 REPLICATION CLIENT 服务器管理 复制权限 REPLICATION SLAVE 服务器管理 复制权限 SHOW DATABASES 服务器管理 查看数据库权限 SHUTDOWN 服务器管理 关闭数据库权限 SUPER 服务器管理 执行kill线程权限 TCP 图：
该图详细介绍了 TCP/IP 协议族中的各个协议在 OSI 模型中的分布
  ]]></content></entry><entry><title>跨城区局域网的搭建（基于Docker）</title><url>/posts/old/mi-docker-net/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>VPN</tag></tags><content type="html"><![CDATA[  概述： 管理复杂网络内的系统,有时候需要突破网络限制.有比较多的方案,比如ss5,Shadowsocks,vpn等. 这里提供一种方案是利用 docker-openvpn 实施多重复杂网络内的主机互联,实现利用nginx反向代理各类服务.
概念图： 具体流程： 购买云服务器部署docker 建议购买支持systemctl的Linux系统,比较好管理,并部署docker:
外挂存储格式化为xfs分区; 1 2 # mkfs.xfs /dev/vdb5 # echo &#34;/dev/vdb5 /mnt/data xfs defaults 1 1&#34; |tee -a /etc/fstab 调整docker的目录,两种方法; 挂载/var/lib/docker目录: 1 2 3 4 5 6 # systemctl stop docker # mkdir -p /mnt/data/docker # rsync -aXS /var/lib/docker/. /mnt/data/docker/ # echo &#34;/mnt/data/docker /var/lib/docker none bind 0 0&#34;|tee -a /etc/fstab # mount -a # systemctl start docker 指定具体目录: 在/etc/systemd/system/multi-user.target.wants/docker.service里面修改如下: 1 ExecStart=/usr/bin/dockerd --storage-driver=overlay2 -g /mnt/hhd/docker 安装docker-openvpn服务: docker-openvpn Pick a name for the $OVPN_DATA data volume container, it will be created automatically. 1 # OVPN_DATA=&#34;ovpn-data&#34; Initialize the $OVPN_DATA container that will hold the configuration files and certificates 1 2 3 4 5 6 # docker volume create --name $OVPN_DATA # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_genconfig \ -u udp://VPN.SERVERNAME.COM # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -it kylemanna/openvpn ovpn_initpki 如果使用tcp 1 2 3 # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_genconfig \ -u tcp://VPN.SERVERNAME.COM:1443 Start OpenVPN server process 1 2 3 # docker run -v $OVPN_DATA:/etc/openvpn -d \ -p 1194:1194/udp \ --cap-add=NET_ADMIN kylemanna/openvpn OR 1 2 3 # docker run -v $OVPN_DATA:/etc/openvpn -d \ -p 1443:1194/tcp \ --cap-add=NET_ADMIN kylemanna/openvpn Running a Second Fallback TCP Container 1 2 3 4 # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -p 1443:1194/tcp \ --privileged kylemanna/openvpn ovpn_run \ --proto tcp Generate a client certificate without a passphrase . Retrieve the client configuration with embedded certificates, &ldquo;CLIENTNAME&quot;可自定义; 1 2 3 4 # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -it kylemanna/openvpn easyrsa build-client-full CLIENTNAME nopass # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_getclient CLIENTNAME &gt; CLIENTNAME.ovpn 增加路由规则 在docker主机上增加一条路由规则,目的是使其他容器可以通过默认的网络来访问到openvpn客户端的节点: 1 # ip route add 192.168.255.0/24 via $DOCKER_OPENVPN_IP 给客户端配置静态内外IP。 1 2 # cat ccd/CLIENTNAME ifconfig-push 192.168.255.10 192.168.255.9 部署前端代理 选择 DOCKER-CADDY 做反向代理: 1 2 3 4 5 6 7 docker run -d \ -v $(pwd)/Caddyfile:/etc/Caddyfile \ -v $HOME/.caddy:/root/.caddy \ -p 80:80 -p 443:443 \ --name caddy \ --link openvpn:openvpn \ abiosoft/caddy CADDY的配置参考： 1 2 3 4 5 6 7 8 9 10 11 http://git.mickeybee.cn { redir https://git.mickeybee.cn{url} } https://git.mickeybee.cn { gzip proxy / 192.168.xxx.xx:3000 tls xxx@xxx.com { max_certs 10 key_type p256 } } 部署TCP代理 选择 DOCKER-HAPROXY 并部署: 1 2 3 4 5 6 docker run -d \ -v $(pwd)/haproxy:/usr/local/etc/haproxy:ro \ -p xxx:xxx -p yyy:yyy \ --name haproxy \ --link openvpn:openvpn \ haproxy 映射后端端口。   ]]></content></entry><entry><title>Tumx + Git + OhMyZsh + VIM</title><url>/posts/old/zsh-tmux-vim-git/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Tumx</tag><tag>Zsh</tag></tags><content type="html"><![CDATA[  Ubuntu下的环境： 要求： tmux &gt;= 2.1 vim &gt;= 7.3 zsh (oh-my-zsh) git 部署环境： TMUX(使用 gpakosz 的配置)： 部署方式： 1 2 3 4 5 $ cd ~ $ git clone https://github.com/gpakosz/.tmux.git $ ln -s -f .tmux/.tmux.conf $ cp .tmux/.tmux.conf.local . $ sudo apt-get install xclip ## Ubuntu下安装xclip来支持跨文件复制粘贴 修改“.tmux.conf” 把以下地方修改： 1 bind -t vi-copy y copy-selection 改为
1 bind -t vi-copy y copy-pipe &#34;xclip -sel clip -i&#34; 如果tmux &lt;1.8 请修改如下：
1 2 3 # copy &amp; paste between tmux and x clipboard bind C-p run-shell &#34;tmux set-buffer \&#34;$(xclip -o)\&#34;; tmux paste-buffer&#34; bind C-y run-shell &#34;tmux show-buffer | xclip -sel clip -i&#34; 修改“.tmux.conf.local”把以下地方注释去掉：
1 2 # set -g status-keys vi # set -g mode-keys vi GIT： 日常都会用到几个git库，有包含同事的和自己的。管理起来都比较麻烦，但是利用到git子模块会比较方便。
1 2 3 4 5 6 7 $ mkdir xxxx $ git init $ git remote add origin ssh://git@git.xxx.net/mickey/xxxx.git $ git submodule add ssh://git@git.xxx.net/mickey/tttt.git xxxx $ git add -A $ git commit -m &#34;add submodule tttt&#34; $ git push --set-upstream origin master 当需要全部更新的时候：
1 $ git submodule foreach git pull 初始化:
1 2 3 4 5 6 ➜ ~ git clone ssh://git@git.xxx.net/mickey/xxxx.git ➜ ~ cd xxxx ➜ xxxx git:(master) git submodule init ➜ xxxx git:(master) git submodule sync ➜ xxxx git:(master) git submodule update ➜ xxxx git:(master) git submodule foreach git pull origin master   ]]></content></entry></search>