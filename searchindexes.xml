<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>指标流聚合系列三：Record Rule维度任务生成器</title><url>/posts/opentelemetry/stream-metrics-three/</url><categories><category>Prometheus</category><category>VictoriaMetrics</category></categories><tags><tag>Prometheus</tag><tag>VictoriaMetrics</tag></tags><content type="html"><![CDATA[  流聚合计算是一种实时处理和分析大量数据流的技术。其主要目的是从高速、连续不断的数据流中提取有价值的信息，例如：计算平均值、求和、最大值、最小值等。流聚合计算通常会在内存中进行，以保持较低的延迟和高吞吐量。
流聚合计算的实现通常包含以下几个关键部分：
数据源和数据接收器：设定和监控数据流，如：日志、网络数据包、传感器数据等，将数据流分为多个小批次。 窗口操作：数据流会被分成一系列固定或滑动窗口。每个窗口内的数据会进行预先定义的聚合操作。 聚合函数：对窗口内的数据进行各种操作,例如：计数、求和、均值、最大/最小值等。 输出：将聚合结果输出或存储，以备进一步处理或实时数据可视化。 在指标流聚合中，该能力是对单一指标做高效的降维计算，但是在现实中要描述复杂场景是需要对多指标来结合函数计算的。
流聚合可以高效解决以下聚合过滤req_path的字段
聚合前： 1 2 3 4 5 6 7 a_http_req_total{zone=&amp;#34;bj&amp;#34;,src_svr=&amp;#34;192.168.1.2&amp;#34;,src_port=&amp;#34;30021&amp;#34;,dis_svr=&amp;#34;192.168.2.3&amp;#34;,dis_port=&amp;#34;8080&amp;#34;,code=&amp;#34;202&amp;#34;,req_path=&amp;#34;/xxxx/yyyy?abc=exg&amp;#34;} a_http_req_total{zone=&amp;#34;bj&amp;#34;,src_svr=&amp;#34;192.168.1.2&amp;#34;,src_port=&amp;#34;30023&amp;#34;,dis_svr=&amp;#34;192.168.2.3&amp;#34;,dis_port=&amp;#34;8080&amp;#34;,code=&amp;#34;202&amp;#34;,req_path=&amp;#34;/xxxx/yyyy?abc=sdf&amp;#34;} …  ]]></content></entry><entry><title>指标流聚合系列二：分布式流聚合的网关设计与实现</title><url>/posts/opentelemetry/stream-metrics-two/</url><categories><category>Prometheus</category><category>VictoriaMetrics</category></categories><tags><tag>Prometheus</tag><tag>VictoriaMetrics</tag></tags><content type="html"><![CDATA[  通过《指标流聚合系列一：社区VM对指标流聚合能力分析与问题》分析了目前业界方案结合实际场景遇到的问题分析后，这一节会有针对性的对问题一一解决。 通过源码分析有两个关键部分：
时间窗口的范围： 1 2 3 //窗口的范围是间隔设置加上间隔向右位移1位 currentTime := fasttime.UnixTimestamp() deleteDeadline := currentTime + as.intervalSecs + (as.intervalSecs &amp;gt;&amp;gt; 1) 计算窗口的逻辑： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // 从缓存读取需聚合的指标 v, ok := as.m.Load(outputKey) if !ok { v = &amp;amp;totalStateValue{ lastValues: make(map[string]*lastValueState), } vNew, loaded := as.m.LoadOrStore(outputKey, v) if loaded { v = vNew } } sv := v.(*totalStateValue) sv.mu.Lock() deleted := sv.deleted if !deleted { // 从聚合后指标的缓存里面查找流入的时间线的记录值 lv, ok := sv.lastValues[inputKey] if !ok { // 如果没有记录值，则取原始值作为记录值 lv = &amp;amp;lastValueState{} sv.lastValues[inputKey] = lv } d := value if ok &amp;amp;&amp;amp; lv.value &amp;lt;= value { // 如果有记录值，则取新值与记录值的差值作为聚合后指标的追加值 d = value - lv.value } if ok || currentTime &amp;gt; as.ignoreInputDeadline { sv.total += d } lv.value = value lv.deleteDeadline = deleteDeadline …  ]]></content></entry><entry><title>指标流聚合系列一：社区VM对指标流聚合能力分析与问题</title><url>/posts/opentelemetry/stream-metrics-one/</url><categories><category>Prometheus</category><category>VictoriaMetrics</category></categories><tags><tag>Prometheus</tag><tag>VictoriaMetrics</tag></tags><content type="html"><![CDATA[  一、VictoriaMetrics开源项目的原生能力 VictoriaMetrics项目中的流聚合能力是从1.86版本开始整合到vmagent的，具体可参考： https://github.com/VictoriaMetrics/VictoriaMetrics/issues/3460 从源码分析来看，流集合能力如图：
核心计算的代码在pushSample函数中有描述：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 func (as *totalAggrState) pushSample(inputKey, outputKey string, value float64) { currentTime := fasttime.UnixTimestamp() deleteDeadline := currentTime + as.intervalSecs + (as.intervalSecs &gt;&gt; 1) again: v, ok := as.m.Load(outputKey) if !ok { v = &amp;totalStateValue{ lastValues: make(map[string]*lastValueState), } vNew, loaded := as.m.LoadOrStore(outputKey, v) if loaded { v = vNew } } sv := v.(*totalStateValue) sv.mu.Lock() deleted := sv.deleted if !deleted { lv, ok := sv.lastValues[inputKey] if !ok { lv = &amp;lastValueState{} sv.lastValues[inputKey] = lv } d := value if ok &amp;&amp; lv.value &lt;= value { d = value - lv.value } if ok || currentTime &gt; as.ignoreInputDeadline { sv.total += d } lv.value = value lv.deleteDeadline = deleteDeadline sv.deleteDeadline = deleteDeadline } sv.mu.Unlock() if deleted { goto again } } 二、流聚合能力的一般运用分析 首先先看看流聚合后的时序图： 这时候，我们看看时序数据的理论形态 基于理论形态的理想流聚合形态 在实际状态下的流聚合形态 三、原生流聚合能力的日常问题 1、采集断点问题 在现实世界是更残酷的，某个时间断点是无可避免，原因很多，有网络上的、有被采集服务的性能问题等。 高精度的流聚合断点问题在大数计算情况下，影响是很极端的： 2、巨量数据的算力架构 流聚合没有历史数据细节的状态保存，因此性能极优，但是再优性能的服务也存在单点的极限值。在处理单点无法处理的数据量级情况下，分布式算力结构就会存在一系列问题要解决：
vmagent自带采集能力是否可用？ 在实际测试过程中，启动vmagent分片加副本采集能力结合实时流聚合，资源使用大幅增加，在量级极大的情况下频繁出现采集连续断点的问题，恶化了断点导致的计算结果差异问题。 架构图参考：
需计算的Sample数据分别给哪个算力点计算？ 多个算力点计算后的同维度指标集因同维度同时间窗口但不同结果值导致后到值丢弃的问题如何解决？（触发乱序指标处理逻辑） 分布式计算的资源均衡问题？ 插入任务id解决计算点同维度分辨问题后再次引入新维度如何解决？   ]]></content></entry><entry><title>闲聊一下CPU时序和现代操作系统二三事</title><url>/posts/opentelemetry/talk-about-cpu-timer/</url><categories><category>Linux</category><category>CPU</category></categories><tags><tag>Linux</tag></tags><content type="html"> 时分系统和Linux 首先我们补习一下时分系统，时分系统是一个非常重要的操作系统概念,它最大限度地提高了运算机的利用率,是实现多道程序并发执行的重要手段。 我们日常工作用到的Linux系统 内核也采用了时分系统的思想,主要体现在以下几个方面:
时间片: Linux 使用时间片机制对 CPU 进行时间分割,每个进程只能执行一个时间片的时间,然后交出 CPU 给其他进程运行。这实现了 CPU 时间的共享与公平分配。
上下文切换: 当时间片用完或进程主动放弃 CPU 时,会进行上下文切换,保存当前进程上下文并恢复下一个进程上下文。这使得 CPU 可以高效地在不同进程间切换。
进程调度: Linux 使用 CFS 调度器根据每个进程的时间片选择最适合运行的进程,这是时分系统思想的体现。不同的调度策略可以实现不同的时分效果。
中断机制: Linux 使用中断机制实现对时间的管理与调度。时钟中断可以在时间片用完时通知内核进行上下文切换与调度。这为时分系统提供了动力基础。
时钟事件: Linux 基于时分系统管理各种时间事件,如定时器、睡眠唤醒等。这需要内核根据时钟中断来进行管理与调度。
除此之外,时分系统的思想在 Linux 中还体现在:
多道程序设计 Linux 支持多道程序并发运行,这也依赖于时分系统实现的 CPU 时间共享机制。 实时性 通过设置实时调度策略和对中断处理的优化,Linux 可以提供较好的实时响应性能。这也需要时分系统的支持。 睡眠唤醒 进程可以主动睡眠释放 CPU,这需要时分系统在其唤醒后重新调度其 CPU 时间。 同步机制 Linux 提供多种同步机制,这都需要时分系统来实现进程之间的协调与调度。 时分系统是 Linux 实现多道程序、并发执行、实时响应、时间管理等功能的基础。它使 Linux 能够充分利用 CPU 资源,实现高效率与公平的调度。时分系统的思想贯穿 Linux 内核的方方面面,是理解 Linux 调度与实现并发执行的重要概念。
聊一下Linux的时序 接下来我们需要提前了解几个概念：
Some Words Jiffies 有更多兴趣的可以看看《 内核时钟问题 》
jiffies是一个非常重要的Linux内核变量,它代表了自系统启动以来的时间戳,以时钟中断的个数来表示。它有以下特点:
jiffies以时钟中断的个数来衡量时间,所以它的精度由 …</content></entry><entry><title>About me</title><url>/about.html</url><categories/><tags/><content type="html"> 在多年工作中，从运维历练到开发，可观测性技术爱好者，对云原生技术也略有经验。 从Shell学起，写了点VBA工具，后来续Python、Lua之后玩起了Golang和Rust。 有过国内外业务平台的运维和架构设计的经历。 用Promtheus+Thanos+VictoriaMetrics+自研架构组件，搭建支持日千亿指标量的跨地域混合云监控平台。
作品 不是很多
Shell+Python 工作需要常写，有写复杂脚本的能力，请点击 MLSBS ,相关的 GIT库 Golang 给Thanos社区和kube-state-metric项目修过Bug，提交了一点点代码
eBPF/Rust 给eBPF项目的代表之一DeepFlow社区的Agent项目补充了MongoDB协议的解码逻辑，用Rust实现。</content></entry><entry><title>Linode的BBR简单测试</title><url>/posts/old/linode-bbr-test/</url><categories><category>Tcp</category></categories><tags><tag>Tcp</tag></tags><content type="html"><![CDATA[  概述: TCP BBR 致力于解决两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟、高带宽的网络链路。 降低网络链路上的 buffer 占用率，从而降低延迟。非常适合慢速接入网络的用户。 测试目的: 这次的测试主要是针对丢包率.更有说服力的测试请参考 Lawrence Brakmo的BBR Report . BBR的另一面 测试准备： ADDR01：aaa.aaa.aaa.aaa 1 2 $ uname -r 4.8.6-x86_64-linode78 ADDR02：bbb.bbb.bbb.bbb 1 2 # uname -r 4.8.6-x86_64-linode78 测试方式： 模拟丢包1%-30%的场景，分别测试不同内核开启BBR先后的情况。 用到的tc指令： 1 2 3 4 5 6 7 8 # 清理tc规则： tc qdisc del root dev eth0 # 模拟1%丢包： tc qdisc add dev eth0 root netem loss 1% # 模拟10%丢包： tc qdisc add dev eth0 root netem loss 10% # 模拟30%丢包： tc qdisc add dev eth0 root netem loss 30% 测试从从ADDR02传数据到ADDR01,ADDR01的内核不变,ADDR02在每次测试都会调整内核重启。 测试过程： 步骤略,test.gz约160MB,过程大致如下: 没有启用BBR的情况，从ADDR02传数据到ADDR01： 1 2 3 4 5 6 $ rsync -ave &#34;ssh -l mickey&#34; --progress test.gz mickey@bbb.bbb.bbb.bbb:/home/mickey/test.gz sending incremental file list test.gz 166042909 100% 3.27MB/s 0:00:48 (xfer#1, to-check=0/1) sent 166063274 bytes received 31 bytes 3288382.28 bytes/sec total size is 166042909 speedup is 1.00 测试数据比对: 4.8.6-x86_64-linode78 4.9.15-x86_64-linode78 非linode的官方4.10内核(generic) 没有启动BBR正常情况 3.27MB/s 3.36MB/s 没有测试 启动BBR正常情况 没有测试 3.45MB/s 2.31MB/s 启动BBR丢包1% 3.19MB/s 没有测试 没有测试 启动BBR丢包10% 没有测试 3.21MB/s 2.81MB/s 启动BBR丢包30% 97.30kB/s(在20分钟内没有传输完成中断得到的最后结果) 1.35MB/s 1.15MB/s 测试总结和当时情况(以上述结果来总结): linode自己编译的内核有明显针对性优化,效果比较明显. 启动bbr后在丢包30%的情况下还能完成传输,bbr的效果也比较明显; 4.10内核选择了generic没有选择lowlatency. 本来还打算测50%的丢包.但是50%设置后几乎无法远程操作ADDR01而放弃测试. 附录： centos7官方内核的升级方法： 升级内核: 1 2 3 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install kernel-ml -y 更新启动 1 2 3 egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \&#39; grub2-set-default 0 #default 0表示第一个内核设置为默认运行, 选择最新内核就对了 reboot 开启BBR： 1 2 3 4 modprobe tcp_bbr echo &#34;net.core.default_qdisc=fq&#34; &gt;&gt; /etc/sysctl.conf echo &#34;net.ipv4.tcp_congestion_control=bbr&#34; &gt;&gt; /etc/sysctl.conf sysctl -p   ]]></content></entry><entry><title>树莓派 RaspberryPi docker集群</title><url>/posts/old/rasp_pi_docker/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>Arm</tag></tags><content type="html"><![CDATA[  概述： 参考 Hypriot 的博客，我买了1块Rasp2代板和2块Rasp3代板。
其中2代默认安装了 Hypriot 的系统。3代板如果您有兴趣可以自己参考 《Building a 64bit Docker OS for the Raspberry Pi 3》 这篇文章编译一套64bit的系统。也可以直接下载作者的 已编译好镜像地址 中压缩包。
网络的问题： 日常的升级或者包安装之类的情况，都会遇到墙的问题。为了避免经常为墙而烦恼的情况，有必要给控制网络出口的路由做些调整。参考 《Shadowsocks + ChnRoute 实现 OpenWRT / LEDE 路由器自动翻墙》 解决墙的烦恼，因为这个不是这篇文的重点，具体细节略。
给每个arm板子在路由上赋予一个静态IP地址。
系统： OS细节： 个人习惯调整一下环境，如zsh,vim等，可以考虑弄个ansible-playbook或者shell脚本来简化一下。 1 2 3 4 5 6 7 # 调整默认文本工具为vim $ update-alternatives --config editor $ dpkg-reconfigure locales # 新建自己的账号并加入到docker组 $ usermod a -G docker xxx # 这步对集群很重要，修改设备的名字，否则后期加入集群会报错 $ sed -i &#34;s/black-pearl/node01-pearl/&#34; /boot/device-init.yaml /etc/hostname Kubernetes(本来我想用这个来管理的，但是我的pi2的docker是最新ce版本，kubeadmin提示不支持。) 参考 《Setup Kubernetes on a Raspberry Pi Cluster easily the official way!》 添加源：
1 2 $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $ echo &#34;deb http://apt.kubernetes.io/ kubernetes-xenial main&#34; &gt; /etc/apt/sources.list.d/kubernetes.list 安装kubeadm
1 $ apt-get update &amp;&amp; apt-get install -y kubeadm 初始化主节点
1 $ kubeadm init --pod-network-cidr 10.244.0.0/16 略。。。
Swarm(选择了这个)
初始化pi2
1 $ docker swarm init 把得到的token信息给每个节点接入来
1 2 3 $ docker swarm join \ --token SWMTKN-1-5pm7otmn3vt9bmjhdfdk2hhgxp1zm9wfcyebl7x4dlkbbqujke-4fmuuxxxxxxxxxhiyqem \ 192.168.xx.2:2377 输出一下node信息：
1 2 3 4 5 $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS mbosd1usr6vfoj2p9zyw6zhau node01-pearl Ready Active sc4vtm5fqbdet8k4vrr38x1fo * master-pearl Ready Active Leader v8uxeq9pc8pzlciing7lol16e node02-pearl Ready Active Docker-Engine的安装: 利用 MickeyZZC / MiAnsibleRules 中的docker-engine ,参考以下playbook:
1 2 3 4 5 6 7 8 #!/usr/bin/env ansible-playbook --- - hosts: all gather_facts: yes sudo: yes roles: - role: docker-engine tags: docker Docker-Container: 参考 MickeyZZC / MiAnsibleRules 其他rules. 每个rules都有变量,可以在自己的ansible项目中给予vars值.   ]]></content></entry><entry><title>ARM的点点滴滴记录</title><url>/posts/old/arm-board-note/</url><categories><category>Arm</category></categories><tags><tag>Arm</tag><tag>Linux</tag></tags><content type="html"><![CDATA[  Raspberry Pi 我用树莓派搭建docker集群环境，参考 Hypriot 的博客
CubieBoard 安装docker
选择Hypriot(截止到2017年3月版本是docker 1.11.1)
Install dependencies 1 $ apt-get install -y apt-transport-https Add respository key 1 $ wget -q https://packagecloud.io/gpg.key -O - | sudo apt-key add - Add repository 1 2 $ echo &amp;#39;deb https://packagecloud.io/Hypriot/Schatzkiste/debian/ jessie main&amp;#39; | sudo tee /etc/apt/sources.list.d/hypriot.list $ apt-get update Install Hypriot 1 2 $ apt-get install -y docker-hypriot $ systemctl enable docker 选择Dockerproject (随官方更新，截止2017年3月，版本17.03-ce)
Install dependencies 1 2 $ sudo apt-get update $ sudo apt-get install apt-transport-https ca-certificates Add repository 1 $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D 用下面的命令将 APT 源添加到 source.list（将其中的 替换为下表“表一：dockerproject 对应表”的值）：
1 $ echo &amp;#34;&amp;lt;REPO&amp;gt;&amp;#34; | sudo tee /etc/apt/sources.list.d/docker.list Install Docker-Engine 1 2 $ apt-get update $ apt-get …  ]]></content></entry><entry><title>GitHub博客的搭建</title><url>/posts/old/blog-mickeyzzc-github/</url><categories><category>GitHub</category></categories><tags><tag>GitHub</tag></tags><content type="html"><![CDATA[  2023年的博客从hexo切换到hugo 工具选用了 hugo 主题选择了 Hugo.NexT CDN选择了 CloudFlare 迁移过程 搭建hugo新环境 1 2 3 4 5 6 7 hugo new site mickeyzzcblog cd mickeyzzcblog git init git submodule add https://github.com/hugo-next/hugo-theme-next.git themes/hugo-theme-next cp themes/hugo-theme-next/exampleSite/config.yaml . vim config.yaml hugo server 构建github ci的workflows 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 name: deploy on: push: workflow_dispatch: schedule: # Runs everyday at 8:00 AM - cron: &#34;0 0 * * *&#34; jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: &#34;latest&#34; extended: true - name: Build Web run: hugo - name: Deploy Web uses: peaceiris/actions-gh-pages@v3 with: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} EXTERNAL_REPOSITORY: mickeyzzc/mickeyzzc.github.io PUBLISH_BRANCH: main PUBLISH_DIR: ./public commit_message: ${{ github.event.head_commit.message }} 2017年的博客建设 工具选用了 hexo 主题选择了 NexT.Mist CDN选择了 CloudFlare 本地搭建hexo环境 详细步骤请参考官方网站，这里只提及过程中的注意点
参考 hexo 在本地安装，我用的是debian系统。 在中国境内经常会遇到墙的问题，建议使用 淘宝的cnpm 。 1 $ npm install -g cnpm --registry=https://registry.npm.taobao.org 然后使用cnpm命令安装hexo-cli
1 $ cnpm install -g hexo-cli 个性化的自己的配置，并且在GitHub上建立xxx.github.io库。 在 hexo 的本地目录source下初始化git库，并在自己的git库上管理。 配置CDN和HTTPS 在 hexo 的本地目录public下创建CNAME文件，内容为你的域名。 注册 CloudFlare ， 把域名的NS切换到 CloudFlare 管理。 在 CloudFlare 的Crypto页中， SSL设置为Flexible。这将允许CDN到github pages之间的访问为http。 CloudFlare 提供Page Rules功能， 可设置路由规则。通过规则中的Always use https选项，可以将用户强制跳转到https。 http://*.mickeyzzc.tech/* 使用Docker构建Blog静态文件 利用DOCKERFILE构建一个本地使用的hexo镜像. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # MAINTAINER MickeyZZC &lt;xxx@xxx.com&gt; # DOCKER-VERSION 1.13.0 # FROM node:6 MAINTAINER MickeyZZC &lt;xxx@xxx.com&gt; RUN cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \ &amp;&amp; npm install -g cnpm --registry=https://registry.npm.taobao.org \ &amp;&amp; cnpm install -g hexo-cli \ &amp;&amp; mkdir -p /home/hexo/public \ &amp;&amp; cd /home/hexo \ &amp;&amp; hexo init \ &amp;&amp; git clone https://github.com/iissnan/hexo-theme-next themes/next \ &amp;&amp; cnpm install hexo-deployer-git --save \ &amp;&amp; git clone https://git.oschina.net/MickeyZZC/MiZDoc.git mickeyblog \ &amp;&amp; cp -f mickeyblog/hexo_config/_config.yml _config.yml \ &amp;&amp; cp -f mickeyblog/hexo_config/themes_next_config.yml themes/next/_config.yml \ &amp;&amp; cp -f mickeyblog/hexo_config/gitconfig.cfg ./.gitconfig \ &amp;&amp; cp -rf mickeyblog/hexo_source/* source/ \ &amp;&amp; rm -rf mickeyblog source/_posts/hello-world.md \ &amp;&amp; chown -R node.node /home/hexo &amp;&amp; chown -R node /usr/local/lib/node_modules/ \ &amp;&amp; echo &#34;blog.mickeyzzc.tech&#34; &gt; /home/hexo/public/CNAME \ &amp;&amp; hexo generate \ &amp;&amp; chown -R node.node /home/hexo ENV HOME /home/hexo WORKDIR /home/hexo EXPOSE 4000 USER node CMD [&#34;hexo&#34;,&#34;server&#34;] 构建后本地运行来调试hexo 1 2 3 docker run --rm -p 4000:4000 \ -v $HOME/migit/miBlog:/home/hexo/source\ -it mickeyzzc/node-hexo 最后打包合并到GIT库管理 1 2 3 # git add -A # git commit -a -s -m &#34;xxxx&#34; # git push   ]]></content></entry><entry><title>监控采集点点记录</title><url>/posts/opentelemetry/monitor-experience/</url><categories><category>Monitor</category></categories><tags><tag>Mysql</tag><tag>Tcp</tag><tag>Linux</tag></tags><content type="html"><![CDATA[  MYSQL的监控 MySQL权限经验原则 权限控制主要是出于安全因素，因此需要遵循一下几个经验原则：
只授予能满足需要的最小权限，防止用户干坏事。比如用户只是需要查询，那就只给select权限就可以了，不要给用户赋予update、insert或者delete权限。 创建用户的时候限制用户的登录主机，一般是限制成指定IP或者内网IP段。 初始化数据库的时候删除没有密码的用户。安装完数据库的时候会自动创建一些用户，这些用户默认没有密码。 为每个用户设置满足密码复杂度的密码。 定期清理不需要的用户。回收权限或者删除用户。 eg:
针对MYSQL的监控需要开监控账号，针对本地监控和远程监控的分别授权；
1 2 GRANT USAGE,PROCESS,REPLICATION CLIENT,REPLICATION SLAVE ON *.* TO &#39;monitor&#39;@&#39;10.12.%&#39; IDENTIFIED BY &#39;xxx&#39;; GRANT USAGE,SUPER,PROCESS,REPLICATION CLIENT,REPLICATION SLAVE ON *.* TO &#39;monitor&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;xxx&#39;; 监控手段 show global status; 查看全局状态
show global variables; 查看全局变量设置
mysqladmin MySQL管理工具
show master status; 查看Master状态
show slave status; 查看Slave状态
show binary logs; 查看二进制日志文件
show engine innodb status\G 查看InnoDB存储引擎状态
show engine myisam status\G 查看MyISAM存储引擎状态
还有通过查看information_schema 这个数据库获取InnoDB存储引擎相关信息
权限表 权限 权限级别 权限说明 CREATE 数据库、表或索引 创建数据库、表或索引权限 DROP 数据库或表 删除数据库或表权限 GRANT OPTION 数据库、表或保存的程序 赋予权限选项 REFERENCES 数据库或表 ALTER 表 更改表，比如添加字段、索引等 DELETE 表 删除数据权限 INDEX 表 索引权限 INSERT 表 插入权限 SELECT 表 查询权限 UPDATE 表 更新权限 CREATE VIEW 视图 创建视图权限 SHOW VIEW 视图 查看视图权限 ALTER ROUTINE 存储过程 更改存储过程权限 CREATE ROUTINE 存储过程 创建存储过程权限 EXECUTE 存储过程 执行存储过程权限 FILE 服务器主机上的文件访问 文件访问权限 CREATE TEMPORARY TABLES 服务器管理 创建临时表权限 LOCK TABLES 服务器管理 锁表权限 CREATE USER 服务器管理 创建用户权限 PROCESS 服务器管理 查看进程权限 RELOAD 服务器管理 执行flush-hosts, flush-logs, flush-privileges, flush-status, flush-tables, flush-threads, refresh, reload等命令的权限 REPLICATION CLIENT 服务器管理 复制权限 REPLICATION SLAVE 服务器管理 复制权限 SHOW DATABASES 服务器管理 查看数据库权限 SHUTDOWN 服务器管理 关闭数据库权限 SUPER 服务器管理 执行kill线程权限 TCP 图：
该图详细介绍了 TCP/IP 协议族中的各个协议在 OSI 模型中的分布
  ]]></content></entry><entry><title>跨城区局域网的搭建（基于Docker）</title><url>/posts/old/mi-docker-net/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag><tag>VPN</tag></tags><content type="html"><![CDATA[  概述： 管理复杂网络内的系统,有时候需要突破网络限制.有比较多的方案,比如ss5,Shadowsocks,vpn等. 这里提供一种方案是利用 docker-openvpn 实施多重复杂网络内的主机互联,实现利用nginx反向代理各类服务.
概念图： 具体流程： 购买云服务器部署docker 建议购买支持systemctl的Linux系统,比较好管理,并部署docker:
外挂存储格式化为xfs分区; 1 2 # mkfs.xfs /dev/vdb5 # echo &#34;/dev/vdb5 /mnt/data xfs defaults 1 1&#34; |tee -a /etc/fstab 调整docker的目录,两种方法; 挂载/var/lib/docker目录: 1 2 3 4 5 6 # systemctl stop docker # mkdir -p /mnt/data/docker # rsync -aXS /var/lib/docker/. /mnt/data/docker/ # echo &#34;/mnt/data/docker /var/lib/docker none bind 0 0&#34;|tee -a /etc/fstab # mount -a # systemctl start docker 指定具体目录: 在/etc/systemd/system/multi-user.target.wants/docker.service里面修改如下: 1 ExecStart=/usr/bin/dockerd --storage-driver=overlay2 -g /mnt/hhd/docker 安装docker-openvpn服务: docker-openvpn Pick a name for the $OVPN_DATA data volume container, it will be created automatically. 1 # OVPN_DATA=&#34;ovpn-data&#34; Initialize the $OVPN_DATA container that will hold the configuration files and certificates 1 2 3 4 5 6 # docker volume create --name $OVPN_DATA # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_genconfig \ -u udp://VPN.SERVERNAME.COM # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -it kylemanna/openvpn ovpn_initpki 如果使用tcp 1 2 3 # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_genconfig \ -u tcp://VPN.SERVERNAME.COM:1443 Start OpenVPN server process 1 2 3 # docker run -v $OVPN_DATA:/etc/openvpn -d \ -p 1194:1194/udp \ --cap-add=NET_ADMIN kylemanna/openvpn OR 1 2 3 # docker run -v $OVPN_DATA:/etc/openvpn -d \ -p 1443:1194/tcp \ --cap-add=NET_ADMIN kylemanna/openvpn Running a Second Fallback TCP Container 1 2 3 4 # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -p 1443:1194/tcp \ --privileged kylemanna/openvpn ovpn_run \ --proto tcp Generate a client certificate without a passphrase . Retrieve the client configuration with embedded certificates, &ldquo;CLIENTNAME&quot;可自定义; 1 2 3 4 # docker run -v $OVPN_DATA:/etc/openvpn \ --rm -it kylemanna/openvpn easyrsa build-client-full CLIENTNAME nopass # docker run -v $OVPN_DATA:/etc/openvpn \ --rm kylemanna/openvpn ovpn_getclient CLIENTNAME &gt; CLIENTNAME.ovpn 增加路由规则 在docker主机上增加一条路由规则,目的是使其他容器可以通过默认的网络来访问到openvpn客户端的节点: 1 # ip route add 192.168.255.0/24 via $DOCKER_OPENVPN_IP 给客户端配置静态内外IP。 1 2 # cat ccd/CLIENTNAME ifconfig-push 192.168.255.10 192.168.255.9 部署前端代理 选择 DOCKER-CADDY 做反向代理: 1 2 3 4 5 6 7 docker run -d \ -v $(pwd)/Caddyfile:/etc/Caddyfile \ -v $HOME/.caddy:/root/.caddy \ -p 80:80 -p 443:443 \ --name caddy \ --link openvpn:openvpn \ abiosoft/caddy CADDY的配置参考： 1 2 3 4 5 6 7 8 9 10 11 http://git.mickeybee.cn { redir https://git.mickeybee.cn{url} } https://git.mickeybee.cn { gzip proxy / 192.168.xxx.xx:3000 tls xxx@xxx.com { max_certs 10 key_type p256 } } 部署TCP代理 选择 DOCKER-HAPROXY 并部署: 1 2 3 4 5 6 docker run -d \ -v $(pwd)/haproxy:/usr/local/etc/haproxy:ro \ -p xxx:xxx -p yyy:yyy \ --name haproxy \ --link openvpn:openvpn \ haproxy 映射后端端口。   ]]></content></entry><entry><title>Tumx + Git + OhMyZsh + VIM</title><url>/posts/old/zsh-tmux-vim-git/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Tumx</tag><tag>Zsh</tag></tags><content type="html"><![CDATA[  Ubuntu下的环境： 要求： tmux &gt;= 2.1 vim &gt;= 7.3 zsh (oh-my-zsh) git 部署环境： TMUX(使用 gpakosz 的配置)： 部署方式： 1 2 3 4 5 $ cd ~ $ git clone https://github.com/gpakosz/.tmux.git $ ln -s -f .tmux/.tmux.conf $ cp .tmux/.tmux.conf.local . $ sudo apt-get install xclip ## Ubuntu下安装xclip来支持跨文件复制粘贴 修改“.tmux.conf” 把以下地方修改： 1 bind -t vi-copy y copy-selection 改为
1 bind -t vi-copy y copy-pipe &#34;xclip -sel clip -i&#34; 如果tmux &lt;1.8 请修改如下：
1 2 3 # copy &amp; paste between tmux and x clipboard bind C-p run-shell &#34;tmux set-buffer \&#34;$(xclip -o)\&#34;; tmux paste-buffer&#34; bind C-y run-shell &#34;tmux show-buffer | xclip -sel clip -i&#34; 修改“.tmux.conf.local”把以下地方注释去掉：
1 2 # set -g status-keys vi # set -g mode-keys vi GIT： 日常都会用到几个git库，有包含同事的和自己的。管理起来都比较麻烦，但是利用到git子模块会比较方便。
1 2 3 4 5 6 7 $ mkdir xxxx $ git init $ git remote add origin ssh://git@git.xxx.net/mickey/xxxx.git $ git submodule add ssh://git@git.xxx.net/mickey/tttt.git xxxx $ git add -A $ git commit -m &#34;add submodule tttt&#34; $ git push --set-upstream origin master 当需要全部更新的时候：
1 $ git submodule foreach git pull 初始化:
1 2 3 4 5 6 ➜ ~ git clone ssh://git@git.xxx.net/mickey/xxxx.git ➜ ~ cd xxxx ➜ xxxx git:(master) git submodule init ➜ xxxx git:(master) git submodule sync ➜ xxxx git:(master) git submodule update ➜ xxxx git:(master) git submodule foreach git pull origin master   ]]></content></entry></search>